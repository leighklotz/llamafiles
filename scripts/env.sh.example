# include-once
if [ -z "${IN_LLM_SH_ENV}" ]; then
IN_LLM_SH_ENV=1

# These are the variables you can set in ./env.sh or in the environment
# If you don't want these to override current env,
# use the bash ":-" variable parameter as in MODEL_TYPE=${MODEL_TYPE:-mistral}

# BATCH_SIZE=
# CONTEXT_LENGTH=
# DEBUG=
# ERROR_OUTPUT=
# FORCE_MODEL_RUNNER=
# GRAMMAR_FILE=
# KEEP_PROMPT_TEMP_FILE=
# LLAMAFILE_MODEL_RUNNER=
# LLM_ADDITIONAL_ARGS=
# LOG_DISABLE=
# MODEL_TYPE=
# NGL=
# N_PREDICT=
# PRINT_STACK_TRACE=
# PRIORITY=
# SCUTTLE_REFERER
# SCUTTLE_USER_AGENT=
# SILENT_PROMPT=
# SUMMARIZE_USER_AGENT=
# SYSTEM_MESSAGE=
# TEMPERATURE=
# VERBOSE=
# VIA_API_CHAT_BASE=
# VIA_API_USE_GRAMMAR=

KEEP_PROMPT_TEMP_FILE="${KEEP_PROMPT_TEMP_FILE:-ERRORS}"
PRINT_STACK_TRACE="${PRINT_STACK_TRACE:-1}"

# if an external llamafile executable is available, prefer that to the built-in ones
# todo: hard to override this with env settings
if [ -e "${LLAMAFILE_MODEL_RUNNER}"] && [ -x "${HOME}/wip/llamafile-bin/bin/llamafile" ];
then
    LLAMAFILE_MODEL_RUNNER="${HOME}/wip/llamafile-bin/bin/llamafile -m "
    export FORCE_MODEL_RUNNER=1
fi

# Set VIA_API_CHAT_BASE appropriately based on hostnames, GPU cards, etc.
if [ -z "${VIA}" ];
then
    if [ -n "$(command -v nvfree)" ];
    then
       N=$(nvfree | awk '{print $1}' | sed -e 's/\..*$//')
       # if local GPU in use, default to use it via=api
       if [ "${N}" -lt 23 ];
       then
	   # todo: what of MODEL_TYPE (set, unset)
	   export VIA=api
       else
	   # if local, GPU not in use, use CLI
	   export VIA=cli
       fi
    fi
fi

# Known host and known network configs
if [ "${HOSTNAME}" == "tensor" ];
then
    export VIA_API_CHAT_BASE="${VIA_API_CHAT_BASE:-http://localhost:5000}"
elif onsubnet 192\.168\.1\. ;
then
    export VIA_API_CHAT_BASE="${VIA_API_CHAT_BASE:-http://tensor-net.example.com:5000}"
elif onsubnet 10\.0\.0\. ;
then
    export VIA_API_CHAT_BASE="${VIA_API_CHAT_BASE:-http://tensor.example.com:5000}"
fi

## # this overlaps with the logic at the start if we do have a gpu.
## # should list use cases to test and make it cleaner before putting it back.
## # If no GPU is available, default to via-api
## if [ -n "${VIA_API_CHAT_BASE}" ] && [ -z "${MODEL_TYPE}" ] && ! nvfree > /dev/null
## then
##   VIA="api"
## fi

unset IN_LLM_SH_ENV
fi
