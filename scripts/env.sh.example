if [ -z "${IN_LLM_SH_ENV}" ]; # include-once
then
IN_LLM_SH_ENV=1
# These are the variables you can set iin ./env.sh or in the environment
# If you don't want these to override current env,
# use the bash ":-" variable parameter as in MODEL_TYPE=${MODEL_TYPE:-mistral}

# BATCH_SIZE=
# CONTEXT_LENGTH=
# DEBUG=
# ERROR_OUTPUT=
# GRAMMAR_FILE=
# LOG_DISABLE=
# MODEL_TYPE=
# NGL=
# N_PREDICT=
# PRIORITY=
# SILENT_PROMPT=
# SYSTEM_MESSAGE=
# TEMPERATURE=
# VERBOSE=
# VIA_API_CHAT_BASE=
# LLM_ADDITIONAL_ARGS
# LLAMAFILE_MODEL_RUNNER=
# FORCE_MODEL_RUNNER=

if [ -x "/home/klotz/wip/llamafile-bin/bin/llamafile" ];
   then
       LLAMAFILE_MODEL_RUNNER="/home/klotz/wip/llamafile-bin/bin/llamafile -m "
       export FORCE_MODEL_RUNNER=${FORCE_MODEL_RUNNER:-1}
fi

# Set VIA_API_CHAT_BASE appropriately based on hostnames, GPU cards, etc.
if [ -n "$(command -v nvfree)" ];
then
   N=$(nvfree | awk '{print $1}' | sed -e 's/\..*$//')
   if [ "${N}" -lt 23 ];
   then
       export MODEL_TYPE=via-api
   fi
fi

if [ "${HOSTNAME}" == "tensor" ];
then
    export VIA_API_CHAT_BASE="${VIA_API_CHAT_BASE:-http://localhost:5000}"
elif onsubnet 192\.168\.50\. ;
then
    export VIA_API_CHAT_BASE="${VIA_API_CHAT_BASE:-http://tensor-psy.klotz.me:5000}"
elif onsubnet 10\.1\.10\. ;
then
    export VIA_API_CHAT_BASE="${VIA_API_CHAT_BASE:-http://tensor.klotz.me:5000}"
fi

# If no GPU is available, default to via-api
if [ -n "${VIA_API_CHAT_BASE}" ] && [ -z "${MODEL_TYPE}" ] && ! nvfree > /dev/null
then
   MODEL_TYPE="via-api"
fi

unset IN_LLM_SH_ENV
fi
